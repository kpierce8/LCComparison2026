{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison Workflow\n",
    "\n",
    "This notebook demonstrates how to compare model predictions, assess accuracy, create ensembles, and generate reports.\n",
    "\n",
    "**Prerequisites**: Trained classifiers and prediction rasters for at least two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "from src.config_schema import CLASS_SCHEMA, CLASS_COLORS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Accuracy Assessment\n",
    "\n",
    "The `AccuracyAssessor` compares predictions against reference points, computing overall accuracy, kappa, F1, and per-class metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.validation.accuracy_assessor import AccuracyAssessor\n",
    "\n",
    "# Example usage (requires actual raster and reference data):\n",
    "# assessor = AccuracyAssessor()\n",
    "# results = assessor.assess(\n",
    "#     prediction='../data/outputs/landcover_prithvi.tif',\n",
    "#     reference='../data/validation/reference_points.gpkg',\n",
    "#     class_field='LC_CLASS',\n",
    "#     output_dir='../data/outputs/accuracy',\n",
    "# )\n",
    "# print(f\"Overall accuracy: {results['overall_accuracy']:.4f}\")\n",
    "# print(f\"Kappa: {results['kappa']:.4f}\")\n",
    "\n",
    "# Demo with synthetic data\n",
    "print('AccuracyAssessor supports:')\n",
    "print('  - Overall accuracy, kappa, macro F1')\n",
    "print('  - Per-class producer/user accuracy and F1')\n",
    "print('  - Confusion matrix export')\n",
    "print('  - Reference point CRS reprojection')\n",
    "print('  - Multi-model comparison and ranking')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Error Analysis\n",
    "\n",
    "Spatial error analysis identifies where and why models fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.validation.error_analysis import (\n",
    "    analyze_class_confusions,\n",
    "    compute_spatial_error_density,\n",
    ")\n",
    "\n",
    "# Demo: analyze confusion patterns from a synthetic confusion matrix\n",
    "confusion = np.array([\n",
    "    [85, 5, 10],   # water: 85 correct, 5 confused with trees, 10 with built\n",
    "    [3, 90, 7],    # trees: 90 correct\n",
    "    [8, 2, 90],    # built: 90 correct\n",
    "])\n",
    "class_names = ['water', 'trees', 'built']\n",
    "\n",
    "analysis = analyze_class_confusions(confusion, class_names)\n",
    "\n",
    "print('Top confused class pairs:')\n",
    "for pair in analysis['top_confusions'][:5]:\n",
    "    print(f\"  {pair['from']} -> {pair['to']}: {pair['count']} ({pair['rate']:.1%})\")\n",
    "\n",
    "print(f\"\\nPer-class omission rates:\")\n",
    "for name, rate in zip(class_names, analysis['omission_rates']):\n",
    "    print(f\"  {name}: {rate:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Report Generation\n",
    "\n",
    "Generate HTML reports with embedded plots for confusion matrices and per-class accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.validation.report_generator import (\n",
    "    plot_confusion_matrix,\n",
    "    plot_per_class_accuracy,\n",
    ")\n",
    "\n",
    "# Plot a confusion matrix\n",
    "class_names = ['water', 'trees', 'built']\n",
    "confusion = np.array([\n",
    "    [85, 5, 10],\n",
    "    [3, 90, 7],\n",
    "    [8, 2, 90],\n",
    "])\n",
    "\n",
    "fig_data = plot_confusion_matrix(confusion, class_names, output_mode='figure')\n",
    "plt.show()\n",
    "\n",
    "# Plot per-class accuracy\n",
    "per_class = {\n",
    "    'water': {'producers_accuracy': 0.85, 'users_accuracy': 0.89, 'f1': 0.87},\n",
    "    'trees': {'producers_accuracy': 0.90, 'users_accuracy': 0.93, 'f1': 0.91},\n",
    "    'built': {'producers_accuracy': 0.90, 'users_accuracy': 0.84, 'f1': 0.87},\n",
    "}\n",
    "\n",
    "fig_data = plot_per_class_accuracy(per_class, output_mode='figure')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Comparison\n",
    "\n",
    "Compare two classification maps pixel-by-pixel to measure spatial agreement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.validation.comparison_metrics import compute_spatial_agreement\n",
    "\n",
    "# Demo with synthetic classification arrays\n",
    "np.random.seed(42)\n",
    "size = (100, 100)\n",
    "pred_a = np.random.randint(0, 7, size=size).astype(np.uint8)\n",
    "pred_b = pred_a.copy()\n",
    "# Introduce 20% disagreement\n",
    "mask = np.random.random(size) < 0.2\n",
    "pred_b[mask] = np.random.randint(0, 7, size=mask.sum()).astype(np.uint8)\n",
    "\n",
    "agreement = compute_spatial_agreement(pred_a, pred_b, nodata=255)\n",
    "\n",
    "print(f\"Agreement: {agreement['agreement_pct']:.1f}%\")\n",
    "print(f\"Valid pixels: {agreement['valid_pixels']:,}\")\n",
    "\n",
    "# Visualize agreement map\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "axes[0].imshow(pred_a, cmap='tab10', vmin=0, vmax=6)\n",
    "axes[0].set_title('Model A')\n",
    "axes[1].imshow(pred_b, cmap='tab10', vmin=0, vmax=6)\n",
    "axes[1].set_title('Model B')\n",
    "agree_map = (pred_a == pred_b).astype(np.uint8)\n",
    "axes[2].imshow(agree_map, cmap='RdYlGn', vmin=0, vmax=1)\n",
    "axes[2].set_title(f'Agreement ({agreement[\"agreement_pct\"]:.0f}%)')\n",
    "for ax in axes:\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Ensemble Methods\n",
    "\n",
    "Combine predictions from multiple models to improve accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.classification.ensemble import EnsembleClassifier\n",
    "\n",
    "# Create 3 synthetic model predictions\n",
    "np.random.seed(42)\n",
    "size = (50, 50)\n",
    "base = np.random.randint(0, 7, size=size).astype(np.uint8)\n",
    "\n",
    "# Models mostly agree but have some noise\n",
    "predictions = {}\n",
    "for name in ['prithvi', 'satlas', 'ssl4eo']:\n",
    "    pred = base.copy()\n",
    "    noise = np.random.random(size) < 0.15\n",
    "    pred[noise] = np.random.randint(0, 7, size=noise.sum()).astype(np.uint8)\n",
    "    predictions[name] = pred\n",
    "\n",
    "ec = EnsembleClassifier()\n",
    "\n",
    "# Majority vote\n",
    "result = ec.majority_vote(predictions)\n",
    "print(f\"Majority vote: {result['valid_pixels']} valid pixels\")\n",
    "print(f\"Mean agreement: {result['mean_agreement']:.3f}\")\n",
    "\n",
    "# Weighted vote\n",
    "weights = {'prithvi': 0.5, 'satlas': 0.3, 'ssl4eo': 0.2}\n",
    "w_result = ec.weighted_vote(predictions, weights)\n",
    "print(f\"\\nWeighted vote: mean confidence = {w_result['mean_confidence']:.3f}\")\n",
    "\n",
    "# Agreement map\n",
    "agreement = ec.compute_agreement_map(predictions)\n",
    "print(f\"\\nFull agreement pixels: {agreement['full_agreement_pct']:.1f}%\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "axes[0].imshow(result['classification'], cmap='tab10', vmin=0, vmax=6)\n",
    "axes[0].set_title('Ensemble (Majority Vote)')\n",
    "axes[1].imshow(result['agreement'], cmap='YlGn', vmin=0, vmax=1)\n",
    "axes[1].set_title('Agreement Map')\n",
    "for ax in axes:\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hierarchical Fusion\n",
    "\n",
    "Merge predictions from different resolution sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.classification.ensemble import HierarchicalFusion\n",
    "\n",
    "# Simulate base (10m, Sentinel-2) and refinement (1m, NAIP) predictions\n",
    "np.random.seed(42)\n",
    "size = (50, 50)\n",
    "base_pred = np.random.randint(0, 7, size=size).astype(np.uint8)\n",
    "refinement_pred = base_pred.copy()\n",
    "\n",
    "# Refinement has some nodata (255) areas\n",
    "refinement_pred[40:, :] = 255\n",
    "# But is more detailed where available\n",
    "refinement_pred[:40, :40] = 3  # grass\n",
    "\n",
    "fusion = HierarchicalFusion(nodata=255)\n",
    "result = fusion.fuse(\n",
    "    base_pred, refinement_pred,\n",
    "    strategy='high_res_priority',\n",
    ")\n",
    "\n",
    "print(f\"Base pixels: {result['base_pct']:.1f}%\")\n",
    "print(f\"Refinement pixels: {result['refinement_pct']:.1f}%\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "axes[0].imshow(base_pred, cmap='tab10', vmin=0, vmax=6)\n",
    "axes[0].set_title('Base (Sentinel-2)')\n",
    "\n",
    "ref_display = refinement_pred.copy().astype(float)\n",
    "ref_display[refinement_pred == 255] = np.nan\n",
    "axes[1].imshow(ref_display, cmap='tab10', vmin=0, vmax=6)\n",
    "axes[1].set_title('Refinement (NAIP)')\n",
    "\n",
    "axes[2].imshow(result['classification'], cmap='tab10', vmin=0, vmax=6)\n",
    "axes[2].set_title('Fused Result')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Resolution Matching\n",
    "\n",
    "When combining rasters of different resolutions, the `ResolutionMatcher` aligns them to a common grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.processing.resolution_matcher import ResolutionMatcher\n",
    "\n",
    "print('ResolutionMatcher capabilities:')\n",
    "print('  - Auto-detect finest resolution across inputs')\n",
    "print('  - Compute intersection or union extents')\n",
    "print('  - Reproject across CRS boundaries')\n",
    "print('  - Nearest neighbor (classification) or bilinear (probability) resampling')\n",
    "print()\n",
    "print('Example usage:')\n",
    "print('  matcher = ResolutionMatcher()')\n",
    "print('  aligned = matcher.align_rasters({')\n",
    "print('      \"sentinel\": \"10m_prediction.tif\",')\n",
    "print('      \"naip\": \"1m_prediction.tif\",')\n",
    "print('  }, output_dir=\"aligned/\")')\n",
    "print('  arrays, meta = matcher.load_aligned_arrays(aligned_paths)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Accuracy assessment** against reference data\n",
    "2. **Error analysis** to identify confusion patterns\n",
    "3. **Report generation** with matplotlib plots\n",
    "4. **Model comparison** with spatial agreement maps\n",
    "5. **Ensemble methods** (majority vote, weighted vote)\n",
    "6. **Hierarchical fusion** for multi-resolution data\n",
    "7. **Resolution matching** for raster alignment\n",
    "\n",
    "For the full CLI workflow, see `docs/workflow.md`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
